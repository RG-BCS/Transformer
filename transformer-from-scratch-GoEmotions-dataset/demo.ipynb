{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Emotion Transformer from Scratch\n",
        "\n",
        "This notebook builds a Transformer Encoder model **completely from scratch** in PyTorch for multi-label emotion classification on the GoEmotions dataset.\n",
        "\n",
        "- Custom Multi-Head Attention  \n",
        "- Custom Positional Encoding  \n",
        "- Full training and evaluation pipeline  \n",
        "- No pretrained transformers or Huggingface models used  \n"
      ],
      "metadata": {
        "id": "VD7e6-ylYMrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and setup\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from model import EmotionsModel\n",
        "from utils import train_transformer_encoder, predict_from_text_or_dataset\n",
        "from dataset import text_processor, train_dl, valid_dl, test_ds, dataset\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 25\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "id": "djYn37AzZwOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Configuration\n",
        "Setting up hyperparameters and initializing the model.\n"
      ],
      "metadata": {
        "id": "Oj7T-Rc9Z7yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "num_layers = 2\n",
        "src_vocab_size = len(text_processor.vocab)\n",
        "embed_size = 128\n",
        "d_out_n_heads = embed_size\n",
        "num_heads = 4\n",
        "ffn_hidden_dim = 4 * embed_size\n",
        "dropout = 0.2\n",
        "learning_rate = 3e-4\n",
        "\n",
        "# Initialize model and send to device\n",
        "model = EmotionsModel(num_layers,src_vocab_size,embed_size,d_out_n_heads,num_heads,ffn_hidden_dim).to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "RzK5PzEmZ-Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Class Imbalance with Weighted Loss\n",
        "\n",
        "Computing positive class weights based on label frequencies in the training and validation splits.\n"
      ],
      "metadata": {
        "id": "HGVpQ7ucaMzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_class_weights(dataset):\n",
        "    \"\"\"\n",
        "    Computes class-wise positive weights for BCEWithLogitsLoss\n",
        "    based on label frequency.\n",
        "    \"\"\"\n",
        "    label_freq = torch.zeros(28)\n",
        "    for split in ['train', 'validation']:\n",
        "        for sample in dataset[split]:\n",
        "            for label in sample['labels']:\n",
        "                label_freq[label] += 1\n",
        "    total = label_freq.sum()\n",
        "    pos_weight = total / (label_freq + 1e-6)  # avoid division by zero\n",
        "    return pos_weight\n",
        "\n",
        "pos_weight = compute_class_weights(dataset).to(device)\n",
        "\n",
        "# Define loss function with class weights\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n"
      ],
      "metadata": {
        "id": "oD6_rPwMaOq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model\n",
        "\n",
        "Training for 10 epochs using the custom `train_transformer_encoder` utility function.\n"
      ],
      "metadata": {
        "id": "3GQv0jS1aUnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 10\n",
        "\n",
        "train_metrics_history, train_loss_history, valid_metrics_history, valid_loss_history = train_transformer_encoder(\n",
        "    model, loss_fn, optimizer, train_dl, valid_dl, NUM_EPOCHS=NUM_EPOCHS\n",
        ")\n"
      ],
      "metadata": {
        "id": "8FqEZiuvaTZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Inference on Example Texts\n",
        "\n",
        "Test the trained model on several example sentences.\n"
      ],
      "metadata": {
        "id": "SrUXXF6VabcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSample text used to test model after training\\n\")\n",
        "\n",
        "sample_texts = [\n",
        "    \"I am so happy and excited about this!\",\n",
        "    \"This makes me really angry and sad.\",\n",
        "    \"I'm feeling a bit anxious but hopeful.\",\n",
        "    \"I'm feeling very sad but also relieved.\"\n",
        "]\n",
        "\n",
        "for i, text in enumerate(sample_texts, 1):\n",
        "    emotions, confidences = predict_from_text_or_dataset(\n",
        "        model, text, text_processor, device=device, threshold=0.85\n",
        "    )\n",
        "    print(f\"Text {i}: {text}\\nâ†’ Predicted emotions: {emotions}\\n\")\n"
      ],
      "metadata": {
        "id": "if5xq_qiac3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Predictions from the Test Set\n",
        "\n",
        "Run inference on 10 random samples from the test set.\n"
      ],
      "metadata": {
        "id": "PLEUt3BwaguY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_from_text_or_dataset(model, test_ds, text_processor, n=10, threshold=0.55)\n"
      ],
      "metadata": {
        "id": "rHjCN_P6akAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization of Training Progress\n",
        "\n",
        "Plotting Macro F1 score over epochs for training and validation sets.\n"
      ],
      "metadata": {
        "id": "LdSYuK45aq64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot([m['f1_macro'] for m in train_metrics_history], label=\"Train F1 Macro\")\n",
        "plt.plot([m['f1_macro'] for m in valid_metrics_history], label=\"Valid F1 Macro\")\n",
        "plt.title(\"Macro F1 Score Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"F1 Macro\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ak5X2uGpauEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "This notebook demonstrated building a Transformer-based multi-label emotion classifier **from scratch**:\n",
        "\n",
        "- Implemented custom Transformer encoder layers without relying on pretrained transformer libraries.  \n",
        "- Handled multi-label classification on GoEmotions dataset with a weighted loss to combat class imbalance.  \n",
        "- Showed inference on both example sentences and random test samples.  \n",
        "- Visualized training progression showing consistent improvement in F1 Macro scores.\n",
        "\n",
        "This approach highlights the power and flexibility of building deep learning models from fundamental building blocks, giving deeper insight into the workings of transformer models applied to emotion recognition.\n",
        "\n",
        "---\n",
        "\n",
        "*Feel free to extend this notebook by experimenting with different hyperparameters, adding more sophisticated data augmentation, or fine-tuning on other datasets!*  \n"
      ],
      "metadata": {
        "id": "BWDthhLdatnh"
      }
    }
  ]
}