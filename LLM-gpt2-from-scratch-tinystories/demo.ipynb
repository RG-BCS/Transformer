{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2 TinyStories Text Generation Demo\n",
        "\n",
        "This notebook demonstrates training a GPT-2 style transformer model from scratch on the TinyStories dataset using various sampling methods for text generation.\n",
        "\n",
        "------\n",
        "\n",
        "## Imports and Setup\n",
        "\n",
        "Import all necessary libraries and modules.\n"
      ],
      "metadata": {
        "id": "1Nz2WIq-68Ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from model import OpenWebTextModel\n",
        "from utils import generate_text_sample, train_gpt_2\n",
        "from dataloader_generator import train_dl, tokenizer, vocab_size, PAD_token, EOS_token, loss_ignore_index, device\n",
        "\n",
        "torch.manual_seed(1)\n"
      ],
      "metadata": {
        "id": "twXHI91h69mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Hyperparameters and Initialization\n",
        "\n",
        "Set model hyperparameters and initialize the GPT-2 style transformer model.\n"
      ],
      "metadata": {
        "id": "2D3Mh1po7NQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 1e-4\n",
        "num_epochs = 20\n",
        "num_layers = 12\n",
        "embed_size = 768\n",
        "d_out_n_heads = embed_size\n",
        "ffn_hidden_dim = 4 * embed_size\n",
        "context_length = 256\n",
        "num_heads = 12\n",
        "\n",
        "# Initialize model\n",
        "model = OpenWebTextModel(\n",
        "    num_layers=num_layers,\n",
        "    vocab_size=vocab_size,\n",
        "    embed_size=embed_size,\n",
        "    d_out_n_heads=d_out_n_heads,\n",
        "    num_heads=num_heads,\n",
        "    ffn_hidden_dim=ffn_hidden_dim,\n",
        "    dropout=0.1,\n",
        "    context_length=context_length,\n",
        "    qkv_bias=False,\n",
        "    PAD_token=PAD_token\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "9W4GSrAY7M8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer and Scheduler Setup\n",
        "\n",
        "Create optimizer and learning rate scheduler for training.\n"
      ],
      "metadata": {
        "id": "hkUmJrwE7Wfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "steps_per_epoch = len(train_dl)\n",
        "total_steps = steps_per_epoch * num_epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=500,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=loss_ignore_index)\n"
      ],
      "metadata": {
        "id": "KOFsoJT57ZLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Model\n",
        "\n",
        "Train the GPT-2 model on the TinyStories dataset.\n"
      ],
      "metadata": {
        "id": "GwF8Sq5W7iZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_loss = train_gpt_2(model, train_dl, num_epochs, loss_fn, optimizer, clip_norm=False, max_norm=1.0)\n"
      ],
      "metadata": {
        "id": "iJosKyRL7jvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Training Loss\n",
        "\n",
        "Visualize training loss over epochs.\n"
      ],
      "metadata": {
        "id": "sDi69tJW7mvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(total_loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jTzCcDLr7prs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Generation Samples\n",
        "\n",
        "Generate text samples from the trained model using different decoding methods:\n",
        "\n",
        "- Greedy\n",
        "- Multinomial Sampling\n",
        "- Temperature Sampling\n",
        "- Top-k Sampling\n"
      ],
      "metadata": {
        "id": "BBdPHVZD7sBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ssample_text = [\"It was a dark and stormy night, and the wind howled through the trees\",\n",
        "               \"The machine began to hum as the lights flickered on\",\n",
        "                \"Once upon a time\"]\n",
        "\n",
        "print(f\"\\nSample sentence:\\n\")\n",
        "\n",
        "for text in sample_text:\n",
        "    input_ids = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
        "    methods = [('beam_search', {'beam_width': 3}), ('greedy', {}), ('multinomial', {}),\n",
        "        ('temperature_sampling', {'temperature': 0.8}),('top_k_sampling', {'top_k': 50, 'temperature': 0.7})]\n",
        "\n",
        "    for m, params in methods:\n",
        "        out_ids = generate_text_sample(model, input_ids, max_new_tokens=MAX_LENGTH,methods=m, eos_id=EOS_token, **params)\n",
        "        gen_text = tokenizer.decode(out_ids[0].tolist())\n",
        "        print(f\"=== {m} ===\\n{gen_text}\\n\")\n",
        "    print(\"#\" * 100)\n",
        "    print(\"Next sample prompt begins\")\n",
        "    print(\"#\" * 100)\n",
        "\n",
        "plt.plot(total_loss)\n",
        "plt.xlabel('Epoch');plt.ylabel('Loss');plt.title('Training Loss');plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NV9jzdxc7u81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "In this demo, we successfully trained a GPT-2 transformer model on the TinyStories dataset to generate coherent text sequences. Various decoding strategies were explored, including greedy decoding, multinomial sampling, temperature-based sampling, and top-k sampling.\n",
        "\n",
        "The training loss curve shows the model's learning progress over epochs. The generated samples demonstrate how different sampling methods affect the creativity and randomness of the output:\n",
        "\n",
        "- **Greedy decoding** tends to produce the most deterministic and repetitive outputs.\n",
        "- **Multinomial sampling** introduces randomness which can yield more diverse results.\n",
        "- **Temperature sampling** controls randomness by scaling the logits, balancing between determinism and creativity.\n",
        "- **Top-k sampling** restricts the sampling pool to the top-k most likely tokens, combining diversity with coherence.\n",
        "\n",
        "This notebook serves as a solid foundation for further experimentation with transformer-based language models and text generation techniques.\n"
      ],
      "metadata": {
        "id": "rbRavV2671AM"
      }
    }
  ]
}