{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Machine Translation using Transformer (English → Spanish)**\n",
        "\n",
        "# Introduction:\n",
        "\n",
        " This notebook demonstrates a complete Transformer-based Neural Machine Translation (NMT) system\n",
        " built entirely from scratch using PyTorch. The model translates English sentences into Spanish.\n",
        "\n",
        "# Key Features:\n",
        " - Implements the full Transformer architecture: Encoder, Decoder, Attention, and FFN layers\n",
        " - Uses real parallel sentence data (English-Spanish) from the spa-eng dataset\n",
        " - Includes data preprocessing, tokenization, vocabulary building, and padding\n",
        " - Trains using cross-entropy loss and the Adam optimizer with a learning rate scheduler\n",
        " - Performs greedy decoding to generate translations at inference time\n",
        "\n",
        " This project is designed for educational purposes to provide a deep understanding of how the Transformer model works internally — without relying on external libraries like Hugging Face.\n"
      ],
      "metadata": {
        "id": "L4VsuveWO7ea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup"
      ],
      "metadata": {
        "id": "O-RA42mCPS4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "\n",
        "from dataloader_generator import (\n",
        "    prepareData, load_and_preprocess_data, TranslationDataset,\n",
        "    collate_batch, PAD_token, SOS_token, EOS_token, MAX_LENGTH\n",
        ")\n",
        "from model import Transformer\n",
        "from utils import train_transformer\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "torch.manual_seed(13)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "RRYL5OQ7PJXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Hyperparameters"
      ],
      "metadata": {
        "id": "r1eHaoWbPYBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 2000\n",
        "\n",
        "num_layers = 2\n",
        "embed_size = 256  # Try 512 for better performance on GPU\n",
        "d_out_n_heads = embed_size\n",
        "ffn_hidden_dim = 4 * embed_size\n",
        "num_heads = 4  # Should divide d_out_n_heads evenly\n"
      ],
      "metadata": {
        "id": "pV1QiD86PdDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Dataset Preparation"
      ],
      "metadata": {
        "id": "xhNcSxC1Pg7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load, normalize and tokenize data\n",
        "text_pairs = load_and_preprocess_data()\n",
        "\n",
        "# Remove duplicates\n",
        "seen_eng = set()\n",
        "unique_text_pairs = []\n",
        "for eng, spa in text_pairs:\n",
        "    if eng not in seen_eng:\n",
        "        unique_text_pairs.append((eng, spa))\n",
        "        seen_eng.add(eng)\n",
        "text_pairs = unique_text_pairs\n",
        "\n",
        "# Prepare vocabulary and pairs\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'spa', text_pairs)\n",
        "dataset = TranslationDataset(pairs, input_lang, output_lang)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dl = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n"
      ],
      "metadata": {
        "id": "V1S-gbzIPla_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Model Initialization"
      ],
      "metadata": {
        "id": "4X9t9lvJPuF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size, target_vocab_size = input_lang.n_words, output_lang.n_words\n",
        "\n",
        "# Initialize Transformer model\n",
        "transformer_model = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    src_vocab_size=src_vocab_size,\n",
        "    target_vocab_size=target_vocab_size,\n",
        "    embed_size=embed_size,\n",
        "    d_out_n_heads=d_out_n_heads,\n",
        "    num_heads=num_heads,\n",
        "    ffn_hidden_dim=ffn_hidden_dim,\n",
        "    dropout=0.5,\n",
        "    context_length=MAX_LENGTH,\n",
        "    qkv_bias=False,\n",
        "    PAD_token=PAD_token\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "mwCg8XlYPvyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Loss, Optimizer & Scheduler"
      ],
      "metadata": {
        "id": "CRP-MfICPzV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_token)\n",
        "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=learning_rate)\n",
        "\n",
        "total_steps = len(train_dl) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=int(0.1 * total_steps),\n",
        "    num_training_steps=total_steps\n",
        ")\n"
      ],
      "metadata": {
        "id": "HIKHz0j7P4VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Train the Transformer\n",
        "\n"
      ],
      "metadata": {
        "id": "AgQb9ktEP6kD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = train_transformer(\n",
        "    transformer_model,\n",
        "    train_dl,\n",
        "    num_epochs,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    device,\n",
        "    input_lang,\n",
        "    output_lang,\n",
        "    clip_norm=True,\n",
        "    max_norm=1.0,\n",
        "    MAX_LENGTH=MAX_LENGTH,\n",
        "    SOS_token=SOS_token,\n",
        "    EOS_token=EOS_token\n",
        ")\n"
      ],
      "metadata": {
        "id": "19VB-ZRbQAHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Loss Curve"
      ],
      "metadata": {
        "id": "kXAdd2TtQDTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(train_loss)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.title(\"Transformer Training Loss vs. Epoch\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5v20PpU-QHyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Translation Examples"
      ],
      "metadata": {
        "id": "NY5mty-hQKtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample translations:\")\n",
        "for _ in range(10):\n",
        "    eng, spa = random.choice(text_pairs)\n",
        "    print(f\"Input: {eng}\")\n",
        "    print(f\"Target: {spa}\")\n",
        "    result = transformer_model.generate(\n",
        "        eng,\n",
        "        input_lang,\n",
        "        output_lang,\n",
        "        max_len=MAX_LENGTH,\n",
        "        SOS_token=SOS_token,\n",
        "        EOS_token=EOS_token\n",
        "    )\n",
        "    predicted_tokens = result['output']\n",
        "    predicted_sentence = \" \".join([output_lang.index2word.get(idx, 'UNK') for idx in predicted_tokens])\n",
        "    print(f\"Predicted: {predicted_sentence}\")\n",
        "    print(\"-\" * 80)\n"
      ],
      "metadata": {
        "id": "9U2AJ9WwQOMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "\n",
        "- We implemented a Transformer model from scratch for English-to-Spanish translation.\n",
        " - The model includes core Transformer components:\n",
        "   * Positional Encoding\n",
        "   * Multi-head Self-Attention\n",
        "   * Cross-Attention (Decoder to Encoder)\n",
        "   * Feed Forward Networks\n",
        "   * Layer Normalization and Dropout\n",
        " - Training was done with Adam optimizer, learning rate scheduler, and gradient clipping.\n",
        " - Greedy decoding was used to generate predictions from the trained model.\n",
        " - The model converged steadily as shown by the loss plot.\n",
        " - It demonstrates solid performance on many common translation examples.\n",
        "\n",
        " Future Improvements:\n",
        " - Implement beam search decoding for better translation diversity.\n",
        " - Train on larger datasets for broader generalization.\n",
        " - Add label smoothing to improve generalization.\n",
        " - Incorporate model checkpointing for save/load support.\n",
        " - Explore multilingual support and pretraining methods (e.g., masked language modeling).\n"
      ],
      "metadata": {
        "id": "FvpLJAG7RbG7"
      }
    }
  ]
}