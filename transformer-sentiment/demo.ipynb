{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Demo: Sentiment Analysis with a Custom Transformer Encoder from Scratch\n"
      ],
      "metadata": {
        "id": "KLfAWJ9Nmxc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook, we demonstrate how to build, train, and evaluate a **transformer encoder-based sentiment analysis model**, developed entirely **from scratch** using PyTorch.\n",
        "\n",
        "This model is designed to classify movie reviews as positive or negative. Key features include:\n",
        "\n",
        "- **Custom multi-head self-attention layers** with support for **causal masking** and **padding masks**, enabling the model to handle variable-length input sequences effectively.\n",
        "- **Positional encoding** added to input embeddings to retain word order information.\n",
        "- A **feed-forward network** integrated within each encoder block.\n",
        "- End-to-end training pipeline with loss tracking and accuracy evaluation.\n",
        "\n",
        "By the end of this notebook, you will see how the model learns to predict sentiment and how to use it to infer on new reviews.\n",
        "\n"
      ],
      "metadata": {
        "id": "zn4PAuKXm1if"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup & Imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import your implemented model and utility functions\n",
        "from model import Sentiment_Model\n",
        "from utils import train_transformer_encoder, plot_confusion_matrix, predict_sentiment\n",
        "from dataloader_generator import train_dl, valid_dl, vocab, tokenizer, SEED\n",
        "\n",
        "# Set device and fix seed for reproducibility\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(SEED)\n"
      ],
      "metadata": {
        "id": "uK7L9E4Fm0_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model and Training Setup\n",
        "\n",
        "Here we define hyperparameters and initialize the transformer encoder model.\n",
        "\n",
        "- `num_layers`: number of encoder layers stacked\n",
        "- `embed_size`: embedding dimension of tokens\n",
        "- `num_heads`: number of attention heads for multi-head attention\n",
        "- `ffn_hidden_dim`: dimension of feed-forward network inside the encoder block\n",
        "- `dropout`: dropout rate for regularization\n",
        "\n",
        "We also initialize the optimizer and the binary cross-entropy loss function appropriate for sentiment classification.\n"
      ],
      "metadata": {
        "id": "1fuQU5sPnIdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 10\n",
        "num_layers = 2\n",
        "src_vocab_size = len(vocab)\n",
        "embed_size = 128\n",
        "d_out_n_heads = embed_size\n",
        "num_heads = 4\n",
        "ffn_hidden_dim = 2 * embed_size\n",
        "dropout = 0.4\n",
        "\n",
        "model = Sentiment_Model(num_layers, src_vocab_size, embed_size, d_out_n_heads, num_heads, ffn_hidden_dim).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.BCELoss()\n"
      ],
      "metadata": {
        "id": "vw4ibUnAnPEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop\n",
        "\n",
        "We train the model for a set number of epochs, recording training and validation accuracy and loss at each epoch.\n",
        "\n",
        "This allows us to monitor learning progress and diagnose potential overfitting or underfitting.\n"
      ],
      "metadata": {
        "id": "l_DwiT8mnUqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_acc, train_loss, valid_acc, valid_loss = train_transformer_encoder(\n",
        "    model, loss_fn, optimizer, train_dl, valid_dl, NUM_EPOCHS\n",
        ")\n"
      ],
      "metadata": {
        "id": "lsJKfsbDnWlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing Training Progress\n",
        "\n",
        "Plot accuracy and loss curves for both training and validation datasets.\n",
        "\n",
        "This visualization helps to ensure the model is converging and generalizing well.\n"
      ],
      "metadata": {
        "id": "Ul0jCXwonb71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_acc, label='Train Accuracy', color='blue')\n",
        "plt.plot(valid_acc, label='Validation Accuracy', color='red')\n",
        "plt.title(\"Accuracy Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_loss, label='Train Loss', color='blue')\n",
        "plt.plot(valid_loss, label='Validation Loss', color='red')\n",
        "plt.title(\"Loss Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bPDbBf_ungx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion Matrix on Validation Data\n",
        "\n",
        "We plot the confusion matrix to inspect true positives, true negatives, false positives, and false negatives.\n",
        "\n",
        "This gives insights into the types of classification errors the model makes.\n"
      ],
      "metadata": {
        "id": "s21pXConnh38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(model, valid_dl, labels=['Negative', 'Positive'], normalize=True, title='Normalized Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HPvJEuuZnljp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference on New Movie Reviews\n",
        "\n",
        "Finally, we demonstrate the model's ability to predict sentiment on new, unseen reviews.\n",
        "\n",
        "Each review is tokenized, converted to tensor inputs, and passed through the model to output a sentiment classification with confidence score.\n"
      ],
      "metadata": {
        "id": "94ush4kjnp0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = [\n",
        "    \"I absolutely loved this movie! The story was compelling, the acting was top-notch, and the soundtrack gave me chills. Iâ€™d definitely watch it again.\",\n",
        "    \"This was a total waste of time. The plot made no sense, the characters were dull, and the ending was painfully predictable.\",\n",
        "    \"The film had some strong performances and great cinematography, but it was dragged down by a slow-paced and confusing storyline.\",\n",
        "    \"I am really not sure if I like or hate the movie. It was long and I honestly did not get the whole theme or plot of the movie.\"\n",
        "]\n",
        "\n",
        "for i, review in enumerate(reviews, 1):\n",
        "    sentiment, score = predict_sentiment(model, review, vocab, tokenizer)\n",
        "    print(f\"Review {i} - Predicted Sentiment: {sentiment} (Confidence: {score:.4f})\")\n"
      ],
      "metadata": {
        "id": "O4HyddZOntae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "- We successfully built a transformer encoder from scratch that supports **masking** to handle padded sequences and optional causal masking.\n",
        "- The model learns meaningful sentiment representations from movie reviews using **positional encoding** and **multi-head self-attention**.\n",
        "- Training curves and confusion matrix confirm the model's effectiveness on the validation set.\n",
        "- Sample predictions on unseen text demonstrate practical usage of the model.\n",
        "\n",
        "Future improvements could involve experimenting with deeper models, larger vocabularies, more training data, or integrating pretrained embeddings.\n"
      ],
      "metadata": {
        "id": "0FU_WszFn0LX"
      }
    }
  ]
}